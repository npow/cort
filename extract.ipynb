{
 "metadata": {
  "name": "",
  "signature": "sha256:6a9bd9ad2744c609a5aa2ac7e5e46dcb1a9a709be96cb9ecf28fc4a9b69a31ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import glob\n",
      "import joblib\n",
      "import os\n",
      "import sys\n",
      "from cort.core import corpora\n",
      "from cort.core import mention_extractor\n",
      "\n",
      "INPUT_FILE = 'a2e_0001.v2_auto_conll'\n",
      "INPUT_FILES = 'data/conll-2011/v2/data/%s/*.v2_gold_conll'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_definite_descriptions(mention):\n",
      "    L = set()\n",
      "    tokens = mention.attributes['tokens']\n",
      "    pos = mention.attributes['pos']\n",
      "    tokens_filtered = []\n",
      "    pos_filtered = []\n",
      "    for t,p in zip(tokens, pos):\n",
      "        if p != 'JJ':\n",
      "            tokens_filtered.append(t)\n",
      "            pos_filtered.append(p)\n",
      "    tokens = tokens_filtered\n",
      "    pos = pos_filtered\n",
      "    for i in xrange(len(tokens)-1):\n",
      "        if pos[i] == 'DT' and pos[i+1] in ['NN', 'NNS']:\n",
      "            id = mention.attributes['annotated_set_id']\n",
      "            L.add(tokens[i] + ' ' + tokens[i+1])\n",
      "    return L\n",
      "\n",
      "def is_definite_description(mention):\n",
      "    pos = mention.attributes['pos']\n",
      "    pos = filter(lambda x: x != 'JJ', pos)\n",
      "    is_valid = 'NN' in pos or 'NNS' in pos\n",
      "    for p in pos:\n",
      "        if p not in ['DT', 'NN', 'NNS']:\n",
      "            is_valid = False\n",
      "    return is_valid\n",
      "\n",
      "def is_proper_noun(mention):\n",
      "    pos = mention.attributes['pos']\n",
      "    tokens = mention.attributes['tokens']\n",
      "    tokens_filtered = []\n",
      "    pos_filtered = []\n",
      "    for t,p in zip(tokens, pos):\n",
      "        if p != 'JJ':\n",
      "            tokens_filtered.append(t)\n",
      "            pos_filtered.append(p)\n",
      "    tokens = tokens_filtered\n",
      "    pos = pos_filtered    \n",
      "    is_valid = 'NNP' in pos or 'NNPS' in pos\n",
      "    for p in pos:\n",
      "        if p not in ['DT', 'NNP', 'NNPS']:\n",
      "            is_valid = False\n",
      "    return is_valid\n",
      "\n",
      "def stringify(mention):\n",
      "    tokens = mention.attributes['tokens']\n",
      "    if mention.attributes['pos'][0] == 'DT':\n",
      "        tokens = tokens[1:]\n",
      "    return ' '.join(tokens)\n",
      "\n",
      "def process_file(fname, H):\n",
      "    corpus = corpora.Corpus.from_file(\"conll\", open(fname))         \n",
      "    definite_descriptions = set()\n",
      "    for doc in corpus:\n",
      "        if not doc.id in H:\n",
      "            H[doc.id] = { 'all_dds': set(), 'dds': set(), 'pns': set(), 'gt_pns': {}, 'gt_dds': {} }\n",
      "        doc.system_mentions = mention_extractor.extract_system_mentions(doc)\n",
      "        all_definite_descriptions = set()\n",
      "        for mention in doc.system_mentions:\n",
      "            all_definite_descriptions |= extract_definite_descriptions(mention)\n",
      "\n",
      "        proper_nouns = set()\n",
      "        definite_descriptions = set()\n",
      "        for mention in doc.annotated_mentions:\n",
      "            if is_proper_noun(mention):\n",
      "                proper_nouns.add(mention)\n",
      "            if is_definite_description(mention):\n",
      "                definite_descriptions.add(mention)\n",
      "        H[doc.id]['all_dds'] |= all_definite_descriptions\n",
      "        H[doc.id]['dds'] |= set([stringify(mention) for mention in definite_descriptions])\n",
      "        H[doc.id]['pns'] |= set([stringify(mention) for mention in proper_nouns])\n",
      "        for mention in definite_descriptions:\n",
      "            mention_id = mention.attributes['annotated_set_id']\n",
      "            if not mention_id in H[doc.id]['gt_dds']:\n",
      "                H[doc.id]['gt_dds'][mention_id] = set()\n",
      "            H[doc.id]['gt_dds'][mention_id].add(stringify(mention))\n",
      "        for mention in proper_nouns:\n",
      "            mention_id = mention.attributes['annotated_set_id']\n",
      "            if not mention_id in H[doc.id]['gt_pns']:\n",
      "                H[doc.id]['gt_pns'][mention_id] = set()\n",
      "            H[doc.id]['gt_pns'][mention_id].add(stringify(mention))            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_files(prefix):\n",
      "    H = {}\n",
      "    fnames = glob.glob(INPUT_FILES % prefix)\n",
      "    #print fnames\n",
      "    for i,fname in enumerate(fnames):\n",
      "        if i % 50 == 0:\n",
      "            print i\n",
      "        try:\n",
      "            process_file(fname, H)\n",
      "        except TypeError:\n",
      "            print fname\n",
      "    joblib.dump(H, '%s.pkl' % prefix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for p in ['train', 'dev', 'test']:\n",
      "    process_files(p)\n",
      "    dump_files(p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "data/conll-2011/v2/data/dev/abc_0040.v2_gold_conll"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "150"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dump_files(prefix):\n",
      "    H = joblib.load('%s.pkl' % prefix)\n",
      "    cnt = 0\n",
      "    L = set()\n",
      "    for fname in H:\n",
      "        gt_dds = H[fname]['gt_dds']\n",
      "        gt_pns = H[fname]['gt_pns']\n",
      "        for id in gt_dds:\n",
      "            if len(gt_dds[id]) > 0 and id in gt_pns and len(gt_pns[id]) > 0:\n",
      "                cnt += 1\n",
      "                #print \"pns: \", list(gt_pns[id]), \" dds: \", list(gt_dds[id])\n",
      "                L.add(str(list(gt_pns[id])) + ' ' + str(list(gt_dds[id])))\n",
      "    with open('%s_gt.txt' % prefix, 'wb') as f:\n",
      "        for l in L:\n",
      "            f.write('%s\\n' % l)\n",
      "\n",
      "    with open('%s_dds.txt' % prefix, 'wb') as f:\n",
      "        for fname in H:\n",
      "            all_dds = H[fname]['all_dds']\n",
      "            for dd in all_dds:\n",
      "                f.write('%s\\n' % dd)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    }
   ],
   "metadata": {}
  }
 ]
}